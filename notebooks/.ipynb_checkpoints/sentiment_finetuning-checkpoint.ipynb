{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üçΩÔ∏è Sentiment Analysis Fine-tuning for Restaurant Reviews\n",
    "\n",
    "This notebook fine-tunes DistilBERT for sentiment analysis on Zomato restaurant reviews.\n",
    "\n",
    "**Steps:**\n",
    "1. Load and preprocess Zomato reviews\n",
    "2. Create balanced dataset with proper labels\n",
    "3. Fine-tune DistilBERT\n",
    "4. Save model to `models/sentiment/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets evaluate accelerate pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "RAW_DATA_PATH = 'data/raw/zomato 2.csv'\n",
    "MODEL_SAVE_PATH = 'models/sentiment/final_model'\n",
    "\n",
    "# Model config\n",
    "MODEL_CHECKPOINT = 'distilbert-base-uncased'\n",
    "\n",
    "# Training config\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "SAMPLE_SIZE = 10000  # Number of restaurants to sample\n",
    "\n",
    "# Labels (binary classification like movie reviews)\n",
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\": 0, \"Positive\": 1}\n",
    "\n",
    "print(f\"Model checkpoint: {MODEL_CHECKPOINT}\")\n",
    "print(f\"Save path: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Preprocess Zomato Reviews\n",
    "\n",
    "The `reviews_list` column contains a list of tuples: `[(\"Rated X.X\", \"review text\"), ...]`\n",
    "\n",
    "We need to:\n",
    "1. Parse the string representation to actual list\n",
    "2. Extract rating and review text\n",
    "3. Convert rating to binary label: ‚â•4 ‚Üí Positive, <3 ‚Üí Negative (skip neutral 3-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "print(\"Loading raw data...\")\n",
    "df = pd.read_csv(RAW_DATA_PATH)\n",
    "print(f\"Total restaurants: {len(df):,}\")\n",
    "\n",
    "# Sample for faster processing\n",
    "if SAMPLE_SIZE and SAMPLE_SIZE < len(df):\n",
    "    df = df.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "    print(f\"Sampled: {len(df):,} restaurants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reviews(reviews_str):\n",
    "    \"\"\"\n",
    "    Parse the reviews_list column and extract (rating, text) pairs.\n",
    "    \"\"\"\n",
    "    if pd.isna(reviews_str) or reviews_str == '[]':\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Safely evaluate the string representation\n",
    "        reviews = ast.literal_eval(str(reviews_str))\n",
    "        \n",
    "        parsed = []\n",
    "        for rating_str, review_text in reviews:\n",
    "            # Extract numeric rating from \"Rated 4.0\"\n",
    "            rating_match = re.search(r'(\\d+\\.?\\d*)', rating_str)\n",
    "            if not rating_match:\n",
    "                continue\n",
    "            rating = float(rating_match.group(1))\n",
    "            \n",
    "            # Clean review text - remove \"RATED\\n\" prefix\n",
    "            clean_text = str(review_text).replace('RATED\\n', '').replace('RATED\\\\n', '').strip()\n",
    "            \n",
    "            # Skip empty or very short reviews\n",
    "            if not clean_text or len(clean_text) < 20:\n",
    "                continue\n",
    "            \n",
    "            parsed.append((rating, clean_text))\n",
    "        \n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "# Test parsing\n",
    "sample_reviews = df['reviews_list'].iloc[0]\n",
    "parsed = parse_reviews(sample_reviews)\n",
    "print(f\"Sample parsed reviews: {len(parsed)} reviews\")\n",
    "if parsed:\n",
    "    print(f\"First review: Rating={parsed[0][0]}, Text={parsed[0][1][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_reviews(df):\n",
    "    \"\"\"\n",
    "    Extract all reviews from dataframe and create labeled dataset.\n",
    "    Binary labels: rating >= 4 ‚Üí Positive (1), rating < 3 ‚Üí Negative (0)\n",
    "    We skip neutral reviews (3-4) to get cleaner training signal.\n",
    "    \"\"\"\n",
    "    all_reviews = []\n",
    "    \n",
    "    print(\"Extracting reviews from dataset...\")\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        reviews = parse_reviews(row['reviews_list'])\n",
    "        \n",
    "        for rating, text in reviews:\n",
    "            # Binary labeling (skip 3-4 range for clearer signal)\n",
    "            if rating >= 4:\n",
    "                label = 1  # Positive\n",
    "            elif rating < 3:\n",
    "                label = 0  # Negative\n",
    "            else:\n",
    "                continue  # Skip neutral (3-4)\n",
    "            \n",
    "            # Truncate very long reviews\n",
    "            text = text[:1000]\n",
    "            \n",
    "            all_reviews.append({\n",
    "                'text': text,\n",
    "                'label': label\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(all_reviews)\n",
    "\n",
    "# Extract all reviews\n",
    "reviews_df = extract_all_reviews(df)\n",
    "print(f\"\\nTotal reviews extracted: {len(reviews_df):,}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"  Positive (1): {(reviews_df['label'] == 1).sum():,}\")\n",
    "print(f\"  Negative (0): {(reviews_df['label'] == 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset by undersampling the majority class\n",
    "def balance_dataset(df):\n",
    "    positive = df[df['label'] == 1]\n",
    "    negative = df[df['label'] == 0]\n",
    "    \n",
    "    min_samples = min(len(positive), len(negative))\n",
    "    print(f\"Balancing to {min_samples:,} samples per class\")\n",
    "    \n",
    "    positive_sampled = positive.sample(n=min_samples, random_state=42)\n",
    "    negative_sampled = negative.sample(n=min_samples, random_state=42)\n",
    "    \n",
    "    balanced = pd.concat([positive_sampled, negative_sampled])\n",
    "    balanced = balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    return balanced\n",
    "\n",
    "balanced_df = balance_dataset(reviews_df)\n",
    "print(f\"\\nBalanced dataset size: {len(balanced_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View sample data\n",
    "print(\"Sample positive review:\")\n",
    "pos_sample = balanced_df[balanced_df['label'] == 1].iloc[0]\n",
    "print(f\"  {pos_sample['text'][:200]}...\")\n",
    "\n",
    "print(\"\\nSample negative review:\")\n",
    "neg_sample = balanced_df[balanced_df['label'] == 0].iloc[0]\n",
    "print(f\"  {neg_sample['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    balanced_df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=balanced_df['label']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Validation samples: {len(val_df):,}\")\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['text', 'label']])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display % of training data with label=1\n",
    "np.array(dataset['train']['label']).sum() / len(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT, \n",
    "    num_labels=2, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, add_prefix_space=True)\n",
    "\n",
    "# Add pad token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    text = examples[\"text\"]\n",
    "    \n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Untrained Model (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample texts\n",
    "text_list = [\n",
    "    \"The food was absolutely delicious! Best restaurant ever.\",\n",
    "    \"Terrible experience. Food was cold and service was rude.\",\n",
    "    \"Amazing ambiance and the pasta was incredible.\",\n",
    "    \"Worst biryani I've ever had. Never coming back.\",\n",
    "    \"Loved the desserts! Will definitely visit again.\"\n",
    "]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    \n",
    "    print(f\"  {id2label[prediction]}: {text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_SAVE_PATH + \"_checkpoints\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=50,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",  # Disable wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  Accuracy: {eval_results['eval_accuracy']['accuracy']:.4f}\")\n",
    "print(f\"  Loss: {eval_results['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample texts\n",
    "print(\"Trained model predictions:\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
    "    \n",
    "    # Move to same device as model\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        prediction = torch.argmax(probs, dim=-1).item()\n",
    "        confidence = probs[0][prediction].item()\n",
    "    \n",
    "    print(f\"  {id2label[prediction]} ({confidence:.2%}): {text[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with actual Zomato reviews\n",
    "zomato_test_reviews = [\n",
    "    \"The biryani was amazing! Perfect spices and tender meat. Must visit for biryani lovers.\",\n",
    "    \"Pathetic service. Waited 45 minutes for cold food. Staff was extremely rude.\",\n",
    "    \"Great ambiance for a romantic dinner. The pasta was creamy and delicious.\",\n",
    "    \"Overpriced and underwhelming. The pizza was soggy and tasteless.\",\n",
    "    \"Loved the live music and the cocktails. Perfect weekend hangout spot!\"\n",
    "]\n",
    "\n",
    "print(\"\\nZomato review predictions:\")\n",
    "for text in zomato_test_reviews:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        prediction = torch.argmax(probs, dim=-1).item()\n",
    "        confidence = probs[0][prediction].item()\n",
    "    \n",
    "    print(f\"  {id2label[prediction]} ({confidence:.2%}): {text[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "Path(MODEL_SAVE_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "trainer.save_model(MODEL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "# Save training info\n",
    "import json\n",
    "\n",
    "training_info = {\n",
    "    \"base_model\": MODEL_CHECKPOINT,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"train_samples\": len(train_df),\n",
    "    \"val_samples\": len(val_df),\n",
    "    \"eval_accuracy\": eval_results['eval_accuracy']['accuracy'],\n",
    "    \"id2label\": id2label,\n",
    "    \"label2id\": label2id\n",
    "}\n",
    "\n",
    "with open(f\"{MODEL_SAVE_PATH}/training_info.json\", \"w\") as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Verify Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify the saved model\n",
    "print(\"Loading saved model for verification...\")\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_PATH)\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "# Test prediction\n",
    "test_text = \"This restaurant has the best food I've ever tasted!\"\n",
    "inputs = loaded_tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "\n",
    "print(f\"‚úÖ Loaded model prediction: {id2label[prediction]}\")\n",
    "print(f\"   Test text: {test_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Training Complete!\n",
    "\n",
    "The sentiment model has been saved to `models/sentiment/final_model`\n",
    "\n",
    "You can now use this model in the restaurant recommendation agents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
